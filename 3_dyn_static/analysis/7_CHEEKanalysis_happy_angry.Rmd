---
title: "7_cheek_analysis"
author: "Jen Richmond"
date: "11/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# load packages
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(here)
library(lme4)
library(lmerTest)
library(broom.mixed)
library(pixiedust)
library(beepr)

# note if you run LMM with just the lme4 package you wont get any pvalues 

# loading lmerTest as well gets you pvalues when you test anova(model)
```

# read in data

```{r message=FALSE, warning=FALSE}
df <- read_csv(here("data", "combined", "5_zdiff_binscreened.csv")) 

# fix data types, all chars to factors
df$emotion <- as.factor(df$emotion)

df <- df %>% mutate_if(is.character,as.factor)

glimpse(df)
```


# HAPPY-ANGRY

## CHEEK
Make dataframe with just happy/angry for cheek 
```{r}

dfHA_cheek <- df %>%
  filter(emotion %in% c("626", "727")) %>%
  filter(muscle == "cheek") %>%
  arrange(pp_no, emotion, trial, bin)

glimpse(dfHA_cheek)

```


Mimicry to happy would be evidenced by greater cheek acvitity over time (bin) for happy than angry. 

LMM- looking to predict Zdiff from emotion (happy angry), and bin (1-6) and their interaction, allowing intercepts to vary for participant and trial. 

### Using  lme4 w lmerTest package 

# Model 1 (intercepts only)

#### construct model 

This model predicts Zdiff from fixed effects of emotion (happy, angry), bin (1-10), and emotion x bin interaction. It includes random intercepts for participant (accounting for the potential of some kids to just have more active faces than others) and random intercepts for trials (accounting for the possibiity that face activation differs across the the 10 trials). This model doesn't include slopes (yet).

> It is interesting that when I don't include (1|trial) here  the model doesn't converge. I thought the solution to a model that doesnt converge is to TAKE OUT complexity, not add it in. I kinda want to take trial out (because the output suggest it accounts for a really small amount of variance) but without it the model wont converge



```{r}

lm_model_cheek <- lmer(Zdiff ~ emotion + bin + emotion*bin + (1|pp_no) + (1|trial), data=dfHA_cheek, REML = FALSE) 


```

### test cheek model assumptions

##### plot residuals + qqplot

```{r}

# Check homogeneity of variance assumption
plot(lm_model_cheek)

# Check normality
qqnorm(resid(lm_model_cheek))
qqline(resid(lm_model_cheek))
```

YIKES qqplot not good, the scale of theoretical (-2 to 2) vs. sample (-2 to 10!) is ALL out of WACK. What to do about that....?

### transform to correct normality

Usually we would use a log transform but the Zdiff includes negative values. Alternative = log modulus transform. This transformation log transforms the absolute value (without the -) and then puts the sign back on. Make a new column for log modulus. 
```{r}
dfHA_cheek <- dfHA_cheek %>%
  mutate(log_modulus = sign(Zdiff) * log(1+abs(Zdiff)))
```

#### run new model

```{r}

lm_model_cheek_1 <- lmer(log_modulus ~ emotion + bin + emotion:bin + (1|pp_no), data=dfHA_cheek, REML = FALSE) 

```

#### test assumptions again

Residuals are more evenly distributed than before, qq plot is SO much better (at least the sample quantiles are on the same scale). It is not great but maybe the best we can do.  
```{r}
# Check homogeneity of variance assumption
plot(lm_model_cheek_1)
# Check normality
qqnorm(resid(lm_model_cheek_1))
qqline(resid(lm_model_cheek_1))
```



#### use anova to estimate effects 

```{r}
aov_output <- anova(lm_model_cheek_1) %>%
  rownames_to_column() %>%
  rename(term = rowname)

```

#### use summary to get coefficients 

```{r}
summary(lm_model_cheek_1)

```

#### use tidy to get df
```{r}
tidy_cheek1 <- tidy(lm_model_cheek_1)
```

```{r}

beep(1)
```



# Model 2 (slopes for emotion)

### run model
```{r}
lm_model_cheek_2 <- lmer(log_modulus ~ emotion + bin + emotion*bin + (1 + emotion|pp_no) , data=dfHA_cheek, REML = FALSE) 
                

# random intercepts for pp_no and slopes for emotion
```

### get anova() / summary() / tidy()

```{r}
anova(lm_model_cheek_2)

summary(lm_model_cheek_2)

tidy_cheek2 <- tidy(lm_model_cheek_2)

```
### check fit

#### AIC 
Lower than intercepts only model
```{r}
AIC(lm_model_cheek_1)
AIC(lm_model_cheek_2)

```

#### likelhood ratio test

Use anova to test if there is difference in fit. 

```{r}
anova(lm_model_cheek_1, lm_model_cheek_2)
```

### model 2 take home

Model with random slopes for effect of emotion AND intercepts for participants provides better fit than model with intercepts for participants only. 


```{r}

beep(2)
```



# Model 3 (slopes for emotion and bin)

### run model
```{r}
lm_model_cheek_3 <- lmer(log_modulus ~ emotion + bin + emotion*bin + (1 + emotion + bin|pp_no), data=dfHA_cheek, REML = FALSE) 
                
# random intercepts for pp_no and slopes for emotion and bin
```

### get anova() / summary() / tidy()

```{r}
aov_output3 <- anova(lm_model_cheek_3)

summary(lm_model_cheek_3)

tidy_cheek3 <- tidy(lm_model_cheek_3)


```
### check fit

#### AIC 
Lower than model that includes intercepts and slopes for only emotion main effect. 
```{r}
AIC(lm_model_cheek_2)
AIC(lm_model_cheek_3)

```

#### likelhood ratio test

Use anova to test if there is difference in fit. 

```{r}
anova(lm_model_cheek_2, lm_model_cheek_3)
```
```{r}

beep(3)
```

### model 3 take home

Model with random slopes for effect of emotion and bin provides better fit than model with intercepts and slopes for emotion alone. 


# Model 4 (slopes for emotion and bin and interaction)

### run model
```{r eval = FALSE}
lm_model_cheek_4 <- lmer(log_modulus ~ emotion + bin + emotion*bin + 
                (1 + emotion + bin + emotion*bin|pp_no), 
                data=dfHA_cheek, REML = FALSE) 

# random intercepts for pp_no and slopes for emotion and bin and interaction
```

```{r}
beep(4)
```

### model 4 take home

Model with random slopes for effect of emotion and bin and emotion*bin interaction DOESN'T CONVERGE. Final model = provides better fit than model with intercepts and slopes for emotion alone. 

# BACKtracking LRT (likelihood ratio tests)

As above, when testing whether adding individual random effects improves the model, we probably should have done that in adding main effects and interactions, rather than assuming the best fit for the data will include both main effects and interactions - oops. 

Lets try that... what would the model fit look like if we added main effects of emotion, then bin, then the interaction. 

```{r}
# this model includes only main effect of emotion (and intercepts for participants, slopes for emotion)
cheek_emo <- lmer(log_modulus ~ emotion +
                (1 + emotion|pp_no), data=dfHA_cheek, REML = FALSE) 

beep(1)

```

```{r}

# this model includes main effects of emotion and bin (and intercepts for participants, slopes for both emotion and bin)
cheek_emo_bin <- lmer(log_modulus ~ emotion + bin +
                (1 + emotion + bin|pp_no), data=dfHA_cheek, REML = FALSE)

beep(1)

```

```{r}

# this model includes only main effects of emotion and bin and the emotion*bin interaction (and intercepts for participants, slopes for the main effects of emotion and bin-- AKA the final model above) 


cheek_emo_bin_interaction <- lmer(log_modulus ~ emotion + bin + emotion*bin + (1 + emotion + bin|pp_no), data=dfHA_cheek, REML = FALSE) 
                

beep(1)
```

```{r}
anova(cheek_emo, cheek_emo_bin)

anova(cheek_emo_bin, cheek_emo_bin_interaction)
```

## Tidying model 3 output

```{r}

terms <- c("intercept", "happy vs. angry", "bin1 vs. bin2", "bin2 vs. bin3", "bin3 vs. bin4", "bin4 vs. bin5", "bin5 vs. bin6")

tidier_cheek3 <- tidy_cheek3 %>%
  filter(effect == "fixed") %>%
  select(-group) 


tidier_cheek3 <-  tidier_cheek3 %>%
  mutate(niceterms = c("intercept", "happy vs. angry", "bin1 vs. bin2", "bin2 vs. bin3", "bin3 vs. bin4", "bin4 vs. bin5", "bin5 vs. bin6", "emotion x bin1-2", "emotion x bin2-3", "emotion x bin3-4", "emotion x bin4-5", "emotion x bin5-6"))

```

**UP TO HERE, next to work out how to report the levels of interaction**

# Write up

Linear mixed effects models analysis in R (R core team, 2013), specifically the lme4 (Bates et al., 2014) and lmerTest (Kuznetsova et al., 2014) packages, were used to model the data. The dependent variable was Z score differences between root mean square (RMS) muscle activity during baseline and each of the 100ms bins from 0 to 1000ms post stimulus onset. Due to violations of normality, Z difference scores were transformed using the log modulous transform, which is appropriate when scores to be transformed are both positive and negative. 

Data from the cheek and brow muscle were modelled separately using both fixed and random effects. Fixed effects of emotion (happy, angry), bin (1-10) and their interaction were included along with random intercepts for participant and random slopes for each of the main effects. The addition of random slopes for the main effect of emotion and bin, improved model fit (WHAT TO REPORT HERE RE STAT), however, with the addition of slopes for the emotion x bin interaction the model to fail to converge.  and was not included. The final model reported below include fixed effects testing for the effect of emotion, bin and emotion x bin interaction, with intercepts for participant and slopes for main effects.  

Mimicry is typically evidenced by changes in muscle activity over time that differ in magnitude as a function of emotion, that is a emotion x bin interaction. . In the case of the cheek muscle, mimicry of happy expressions is taken as greater increases in cheek muslce activity over time in response to happy expressions relative to angry expressions. As is illustrated in Figure X, this was indeed the pattern of responding seen. The models showed that including the emotion*bin interaction in the model, significantly improved the fit, relative to the model that included main effects of emotion and bin alone. 

The final model output (displayed in Table X) shows that overall cheek activity was not greater in response to happy expressions than angry expressions (slope estimate = -0.008, 95% CI [Y, Z], t(117.96) = -0.155, p = 0.88). However, muscle activity did significantly increase in magnitude across bins, from the first bin (estimate = 0.123, 95% CI [Y, Z]), t(578.878) = 3.633, p < 0.001). Critically, the magnitude of increases in muscle activity across bins was greater in response to happy expressions than angry expressions, as evidenced by better model fit with the addition of the interaction term (AIC with main effects only = X, AIC with main effects and interaction = Y, model comparison F test = _____). In the first bin, there was no difference in the magnitude of muscle activity as a function of emotion (estimate = -0.08, 95% CI [Y, Z]), t(4092.66) = -1.81, p = 0.07) from bins 2 to 6, activity was significantly greater in response to happy expressions than angry expressions with a peak in the estimate at bin 3 (estimate = -0.21, 95% CI [Y, Z]), t(4092.49) = -4.63, p < 0.01) however, in each subsequent bin the muscle activity was greater in response to happy than acngry expressions. 

```{r}
really_nice_table <- dust(lm_model_cheek_3) %>%
  sprinkle(col = 4:7, round = 3, pad = 15, halign = "center", valign = "middle") %>% 
  sprinkle(col = 8, fn = quote(pvalString(value)), halign = "center", valign = "middle") %>%
  sprinkle_colnames(term = "Term", 
                    estimate = "Estimate", 
                    std.error = "SE",
                    statistic = "t statistic", 
                    p.value = "P-value")  %>%
  sprinkle(bg_pattern_by = "rows") %>%
    sprinkle_print_method("html") 

really_nice_table

```


# Notes re SLOPES + random effects structure

From Meteyard & Davies best practice guidelines there are differing opinions in the field about specififying random effects strucutre. Some say go maximal and then see if that model converges, if not simplify. Under this approach Brauer & Curtin (2018) point to recommendations from Barr et al (2013) re three rules for maximal random effects: 

1. random intercepts for any unit (subject/items) that cause nonindependence
2. random slopes for any within subjects effects
3. random slopes for any interactions that are completely within subjects. 

But such complexity may cause models to not converge. Others suggest that it is impossble to know whether a particular random effects structure is appropriate for a dataset, so it makes more sense to select random effects strcuture according to whether the addition of that effect improves model fit (Matuschek et al 2017). Under this method, intercepts added first, then slopes for main effects, then intercepts and slopes, then interactions (as above).  If the addition of a random effect doesn't improve fit or causes failure to converge then you drop it. 

I think for the EMG analysis, it makes sense to start with the minimal structure (just intercepts for participants, which captures the repeated measures nature of the design)  and add slopes to see if that improves things, if not stick with just intercepts. Can use Likelihood Ratio tests aka anova() to compare models with without added random effects (or AIC/BIC) to determine whether the addition of random effect improves the model. 

From Bates et al (lme4 package paper) https://www.jstatsoft.org/%20article/view/v067i01/

`condition + (condition | subject)` is the same as response  `response ~ condition + (1 + condition | subject)` 

The 1 + refers to intercept which is implied in the left model. The random factor goes on the left of the | (i.e. random slope of condition) and the non independence grouping variables go on the right (i.e. across subjects). 

(1|subject) = random intercept for subjects
(A * B|subject) = random slope for interaction A * B across subjects)

but the intercept is always implied so (1 + A*B|subject) is the same as above. 

# NOTES: regrssion reporting iwth jtools

https://cran.r-project.org/web/packages/jtools/vignettes/summ.html

# NOTES: Tables with pixiedust

Get nice table to appear in rmd. Tried piping tidydf to kable table with kableExtra. Hard to mess with formatting. Playing with the pixiedust package here https://github.com/nutterb/pixiedust

pixiedust is a lot like broom in that it pulls model output into a tidydf but rather than the purpose being for future analysis use (as in broom) the point here is to allow for easy display/formatting. 

The print_method below gets the equivalent of broom::tidy
```{r}
library(pixiedust)

nice_table <- dust(lm_model_cheek_3) %>%
   sprinkle_print_method("html") 
nice_table

```
  
html formatting is a bit too squashed. Other "sprinkles" allow you to change the rounding of specific columns, pad cells to display numbers better, display pvalues appropriately, change column names and add stripes to rows to make it easier to differentiate.                    
```{r}
really_nice_table <- dust(lm_model_cheek_3) %>%
  sprinkle(col = 4:7, round = 3, pad = 15, halign = "center", valign = "middle") %>% 
  sprinkle(col = 8, fn = quote(pvalString(value)), halign = "center", valign = "middle") %>%
  sprinkle_colnames(term = "Term", 
                    estimate = "Estimate", 
                    std.error = "SE",
                    statistic = "t statistic", 
                    p.value = "P-value")  %>%
  sprinkle(bg_pattern_by = "rows") %>%
    sprinkle_print_method("html") 

really_nice_table
```


I wonder if pixiedust sprinkles work with ANOVA table too. 

```{r}
nice_aov <- dust(aov_output) %>%
  sprinkle(col = 1:6, round = 3, pad = 15, halign = "center", valign = "middle") %>% 
  sprinkle(col = 7, fn = quote(pvalString(value)), halign = "center", valign = "middle") %>%
   sprinkle_print_method("html") 

nice_aov

  
```




# NOTES: How to deal with qqplot problem

qqplots for both brow and cheek suggest that normality assumption is violated. log transform wont work in this case because there are negative values, can't log transform negatives. 

A log modulus transform might work. This transform does a log of the absolute value and then puts the sign back. So negative values still end up negative. 

> = sign(x) * log(1+abs(x))

http://www.statsblogs.com/2014/07/14/a-log-transformation-of-positive-and-negative-values/

http://www.jstor.org/stable/pdf/2986305.pdf
